 CPU 设计了两种缓存。第一级缓存或者说是 L1 cache 总是位于 CPU 内部，用来将已解码的指令调入 CPU 的执行引擎。对于那些频繁使用的关键字，多数芯片有第二个 L1 cache 。典型的 L1 cache 的大小为 16 KB。另外，往往还设有二级缓存，也就是 L2 cache，用来存放最近使用过的关键字，一般是兆字节为单位。L1 cache 和 L2 cache 最大的不同在于是否存在延迟。访问 L1 cache 没有任何的延迟，然而访问 L2 cache 会有 1 - 2 个时钟周期的延迟。

>  什么是时钟周期？计算机处理器或 CPU 的速度由时钟周期来确定，该时钟周期是振荡器两个脉冲之间的时间量。一般而言，每秒脉冲数越高，计算机处理器处理信息的速度就越快。 时钟速度以 Hz 为单位测量，通常为兆赫（MHz）或千兆赫（GHz）。 例如，一个4 GHz处理器每秒执行4,000,000,000个时钟周期。
>
> 计算机处理器可以在每个时钟周期执行一条或多条指令，这具体取决于处理器的类型。 早期的计算机处理器和较慢的 CPU 在每个时钟周期只能执行一条指令，而现代处理器在每个时钟周期可以执行多条指令。



**主存**

在上面的层次结构中再下一层是主存，这是内存系统的主力军，主存通常叫做 RAM(Random Access Memory)，由于 1950 年代和 1960 年代的计算机使用微小的可磁化铁氧体磁芯作为主存储器，因此旧时有时将其称为核心存储器。所有不能在高速缓存中得到满足的内存访问请求都会转往主存中。

除了主存之外，许多计算机还具有少量的非易失性随机存取存储器。它们与 RAM 不同，在电源断电后，非易失性随机访问存储器并不会丢失内容。ROM(Read Only Memory) 中的内容一旦存储后就不会再被修改。它非常快而且便宜。（如果有人问你，有没有什么又快又便宜的内存设备，那就是 ROM 了）在计算机中，用于启动计算机的引导加载模块（也就是 bootstrap ）就存放在 ROM 中。另外，一些 I/O 卡也采用 ROM 处理底层设备控制。



**磁盘**

下一个层次是磁盘(硬盘)，磁盘同 RAM 相比，每个二进制位的成本低了两个数量级，而且经常也有两个数量级大的容量。磁盘唯一的问题是随机访问数据时间大约慢了三个数量级。磁盘访问慢的原因是因为磁盘的构造不同



![image-20210824100755415](imgs\749.png)



**虚拟内存**

许多计算机支持一种著名的虚拟内存机制，这种机制使得期望运行的存储空间大于实际的物理存储空间。其方法是**将程序放在磁盘上**，而**将主存作为一部分缓存，用来保存最频繁使用的部分程序**，这种机制需要快速映像内存地址，用来把程序生成的地址转换为有关字节在 RAM 中的物理地址。这种映像由 CPU 中的一个称为 存储器管理单元(Memory Management Unit, MMU) 的部件来完成。



缓存和 MMU 的出现是对系统的性能有很重要的影响，在多道程序系统中，从一个程序切换到另一个程序的机制称为 上下文切换(context switch)，对来自缓存中的资源进行修改并把其写回磁盘是很有必要的。



##### c.IO设备

![image-20210824101344112](imgs\750.png)

如上图所示，这是一个三步的 I/O 过程，第一步，**设备驱动程序会通过写入设备寄存器告诉控制器应该做什么**。然后，**控制器启动设备**。当**控制器完成读取或写入被告知需要传输的字节后，它会在步骤 2 中使用某些总线向中断控制器发送信号**。如果**中断控制器准备好了接收中断信号（如果正忙于一个优先级较高的中断，则可能不会接收），那么它就会在 CPU 的一个引脚上面声明**。这就是步骤3

![image-20210824101438648](imgs\751.png)

在第四步中，中断控制器把该设备的编号放在总线上，这样 CPU 可以读取总线，并且知道哪个设备完成了操作（可能同时有多个设备同时运行）。

一旦 CPU 决定去实施中断后，程序计数器和 PSW 就会被压入到当前堆栈中并且 CPU 会切换到内核态。设备编号可以作为内存的一个引用，用来寻找该设备中断处理程序的地址。这部分内存称作中断向量(interrupt vector)。一旦中断处理程序（中断设备的设备驱动程序的一部分）开始后，它会移除栈中的程序计数器和 PSW 寄存器，并把它们进行保存，然后查询设备的状态。在中断处理程序全部完成后，它会返回到先前用户程序尚未执行的第一条指令，这个过程如下

![image-20210824101514707](imgs\752.png)



##### d.地址空间

每台计算机都有一些主存用来保存正在执行的程序。在一个非常简单的操作系统中，仅仅有一个应用程序运行在内存中。为了运行第二个应用程序，需要把第一个应用程序移除才能把第二个程序装入内存。

复杂一些的操作系统会允许多个应用程序同时装入内存中运行。为了防止应用程序之间相互干扰（包括操作系统），需要有某种保护机制。虽然此机制是在硬件中实现，但却是由操作系统控制的。

上述观点涉及对计算机主存的管理和保护。另一种同等重要并与存储器有关的内容是管理进程的地址空间。通常，每个进程有一些可以使用的地址集合，典型值从 0 开始直到某个最大值。一个进程可拥有的最大地址空间小于主存。在这种情况下，即使进程用完其地址空间，内存也会有足够的内存运行该进程。

但是，在许多 32 位或 64 位地址的计算机中，分别有 2^32 或 2^64 字节的地址空间。如果一个进程有比计算机拥有的主存还大的地址空间，而且该进程希望使用全部的内存，那该怎么处理？在早期的计算机中是无法处理的。但是现在有了一种虚拟内存的技术，正如前面讲到过的，操作系统可以把部分地址空间装入主存，部分留在磁盘上，并且在需要时来回交换它们。

#### 0.进程切换

操作系统为了执行进程间的切换，会维护着一张表格，这张表就是 `进程表(process table)`。每个进程占用一个进程表项。

该表项包含了进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号和调度信息，以及其他在进程由运行态转换到就绪态或阻塞态时所必须保存的信息，从而保证该进程随后能再次启动，就像从未被中断过一样。

![image-20210824103643700](imgs\753.png)

第一列内容与`进程管理`有关，第二列内容与 `存储管理`有关，第三列内容与`文件管理`有关。

现在我们应该对进程表有个大致的了解了，就可以在对单个 CPU 上如何运行多个顺序进程的错觉做更多的解释。与每一 I/O 类相关联的是一个称作 中断向量(interrupt vector) 的位置（靠近内存底部的固定区域）。它包含中断服务程序的入口地址。假设当一个磁盘中断发生时，用户进程 3 正在运行，则中断硬件将程序计数器、程序状态字、有时还有一个或多个寄存器压入堆栈，计算机随即跳转到中断向量所指示的地址。这就是硬件所做的事情。然后软件就随即接管一切剩余的工作。

当中断结束后，操作系统会调用一个 C 程序来处理中断剩下的工作。在完成剩下的工作后，会使某些进程就绪，接着调用调度程序，决定随后运行哪个进程。然后将控制权转移给一段汇编语言代码，为当前的进程装入寄存器值以及内存映射并启动该进程运行，下面显示了中断处理和调度的过程。

- 硬件压入堆栈程序计数器等

- 硬件从中断向量装入新的程序计数器

- 汇编语言过程保存寄存器的值

- 汇编语言过程设置新的堆栈

- C 中断服务器运行（典型的读和缓存写入）

- 调度器决定下面哪个程序先运行 

- C 过程返回至汇编代码 

- 汇编语言过程开始运行新的当前进程

一个进程在执行过程中可能被中断数千次，但关键每次中断后，被中断的进程都返回到与中断发生前完全相同的状态。





#### 0.内存对齐是什么？为什么需要内存对齐

**概念**

内存对齐跟数据在内存中的位置有关，**如果一个变量的内存地址正好处于它长度的整数倍**，它就被称作自然对齐。比如在32位cpu下，假设一个整型变量的地址为0x00000004，那它就是自然对齐的。

**为什么要内存对齐呢？**

**字节对齐主要还是为了提高内存的访问效率，**比如intel 32位cpu，每个总线周期都是从偶地址开始读取32位的内存数据，如果数据存放地址不是从偶数开始，则可能出现需要两个总线周期才能读取到想要的数据，因此需要在内存中存放数据时进行对齐。





#### 1.什么是系统调用呢？

**在介绍系统调用之前，先来了解一下用户态和内核态；**

<u>根据进程访问资源的特点，可以把进程在系统上的运行分为两个级别：</u>

- <u>**用户态：用户态运行的进程可以直接读取用户程序的数据；**</u>
- <u>**内核态：可以理解为在内核态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。**</u>

说了用户态和内核态之后，那么什么是系统调用呢？

<u>我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的内核态的子功能咋办呢，这就需要**系统调用**了。</u>

<u>**也就是说我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等），都必须通过系统调用方式向操作系统提出请求，并由操作系统代为完成。**</u>

这些系统调用功能大致可分为如下几类：

- **设备管理**。完成设备的请求或释放，以及设备启动等功能；
- **文件管理**。完成文件的读、写、创建以及删除等功能；
- **进程控制**。完成进程的创建、撤销、阻塞以及唤醒等功能；
- **进程通信**。完成进程之间的消息传递或信号传递等功能；
- **内存管理**。完成内存的分配、回收以及获取作业占用内存区大小以及地址等功能。

#### 2.用户态转向内核态的过程是怎么样的呢？

- int 0x80- 128

- 保存用户现场；

  - 寄存器压入栈；
  - 进行syscall；
  - 内核态返回eax；
  - 恢复用户态现场；
  - 用户程序继续执行；

  



#### 2.操作系统的内核

- **操作系统的内核(Kernel)是操作系统的核心部分**，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。
- 操作系统的内核是连接应用程序和硬件的桥梁，决定着操作系统的性能和稳定性。

#### 3.中央处理器(CPU,Central Processing Unit)

- CPU是一台计算机的**运算核心(Core) + 控制核心(Control Unit)**，可以称得上是计算机的大脑；
- CPU主要包括两个部分：**控制器+运算器**；
- CPU的根本任务是**执行指令**，对计算机来说最终都是一串由“0”和“1”组成的序列。

#### 4.如何理解CPU和内核Kernel的区别呢

- 操作系统的内核(Kernel)属于操作系统层面，而CPU属于硬件；
- CPU主要提供运算，处理各种指令的能力。内核(Kernel)直呼要负责系统管理比如内存管理，屏蔽了对硬件的操作。

![image-20210419130414675](imgs\85.png)

用户程序、系统调用、内核和硬件之间的关系是什么呢？

![image-20210419130630838](imgs\86.png)







**2.基本功能：**

**进程管理:** 进程控制、进程同步、进程通信、死锁控制；

**内存管理：**内存分配、地址映射、内存保护与共享、虚拟内存地址；

**文件管理：**文件存储空间的管理、目录管理；

**设备管理：**完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率。



**3.基本特征：**

- **并发：** 并发是指宏观上在**一段时间内能同时运行多个程序**；并行指的是**同一时刻能运行多个指令**；**并行需要硬件支持**，如多核处理线；**并发则是操作系统引入进程和线程**，使得程序能够并发运行。
- 共享：共享是指系统中的资源可以被多个并发进程共同使用；有两种共享方式：互斥共享和同时共享；
- 虚拟：把一个物理实体转换为多个逻辑实体；主要有两种虚拟技术：时分复用技术和空分复用技术；
- 异步



**4.中断分类**

- **外中断**：由CPU执行指令以外的事件引起，如I/O中断、控制台中断；
- **异常**：由CPU执行指令的内部事件引起；
- **陷入**：在用户程序中使用系统调用；



**5.计算机操作系统-链接**

- 预处理阶段：处理以#开头的预处理命令；
- 编译阶段：翻译成汇编文件；
- 汇编阶段：将汇编伪码翻译成可重定位目标文件；
- 链接阶段：将可重定位目标文件和printf.o等单独预编译好的目标文件进行合并，得到最终的执行文件。



#### 5.中断  硬中断和软中断

##### 5.0 时间片轮转算法

 时间片轮转（Round-Robin）调度算法是操作系统一种比较公平的进程调度的方式，这种方式使得**就绪队列上的所有进程在每次轮转时都可以运行相同的一个时间片。**

**进程切换时机**

进程在执行时分为两种情况：

- 在**该时间片内进程执行完毕，这种情况调度程序将立即把该进程弹出队列，并把 CPU 分配给新的队首进程**；
- 在该时间片内进程未执行完毕，调度程序将**立即中断该进程执行，把该进程加入队尾，并将 CPU 分配给新的队首进程**

 **时间片大小**

 在 RR 算法中，时间片的大小直接影响了系统的性能。

- 时间片过小，有利于短作业，但是会频繁地切换进程，增加了系统的开销，影响性能。
- 时间片过大，算法退化成 FCFS 算法，如果某个短作业进程之前的进程都是长作业，将导致后面的短作业进程长时间等待。



##### 5.1 中断的过程

从本质上来讲，**中断是一种电信号，当设备有某种事件发生时**，就会产生中断。之后总线通过把**电信号发送给中断控制器**。

如果中断的线是激活的，**中断控制器把电信号发送给处理器的某个特定引脚**，处理器于是立即停止自己正在做的事情。

**跳转到中断处理程序的入口点，进行中断处理**。

![image-20210714220201053](imgs\468.png)

##### 5.2 硬中断

1. 硬中断是由硬件产生的，比如，**像磁盘，网卡，键盘，时钟等。每个设备或设备集都有它自己的IRQ（中断请求**）。基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动上（注：硬件驱动通常是内核中的一个子程序，而不是一个独立的进程）。
2. **处理中断的驱动是需要运行在CPU上的**，因此，当中断产生的时候，**CPU会中断当前正在运行的任务，来处理中断**。在有多核心的系统上，一个中断通常只能中断一颗CPU（也有一种特殊的情况，就是在大型主机上是有硬件通道的，它可以在没有主CPU的支持下，可以同时处理多个中断。）。
3. **硬中断可以直接中断CPU。它会引起内核中相关的代码被触发。**对于那些需要花费一些时间去处理的进程，中断代码本身也可以被其他的硬中断中断。
4. 对于时钟中断，**内核调度代码会将当前正在运行的进程挂起，从而让其他的进程来运行。**它的存在是为了让调度代码（或称为调度器）可以调度多任务。

##### 5.2 软中断

1. 软中断的处理非常像硬中断。然而，它们仅仅是**由当前正在运行的进程所产生的。**
2. 通常，**软中断是一些对I/O的请求**。这些请求会**调用内核中可以调度I/O发生的程序**。对于某些设备，I/O请求需要被立即处理，而磁盘I/O请求通常可以排队并且可以稍后处理。根据I/O模型的不同，进程或许会被挂起直到I/O完成，此时内核调度器就会选择另一个进程去运行。I/O可以在进程之间产生并且调度过程通常和磁盘I/O的方式是相同。
3. 软中断仅与内核相联系。软中断仅与内核相联系。而内核主要负责对需要运行的任何其他的进程进行调度。一些内核允许设备驱动的一些部分存在于用户空间，并且当需要的时候内核也会调度这个进程去运行。一些内核允许设备驱动的一些部分存在于用户空间，并且当需要的时候内核也会调度这个进程去运行。
4. **软中断并不会直接中断CPU。也只有当前正在运行的代码（或进程）才会产生软中断**。这种中断是一种需要内核为正在运行的进程去做一些事情（通常为I/O）的请求。有一个特殊的软中断是Yield调用，它的作用是请求内核调度器去查看是否有一些其他的进程可以运行。

##### 5.3 硬中断与软中断的区别是什么呢？

**硬中断是由外设引发的，软中断是执行中断指令产生的**。

**硬中断的中断号是由中断控制器提供的，软中断的中断号由指令直接指出，无需使用中断控制器。**

**硬中断是可屏蔽的，软中断不可屏蔽。**

**硬中断处理程序要确保它能快速地完成任务，这样程序执行时才不会等待较长时间，称为上半部。**

**软中断处理硬中断未完成的工作，是一种推后执行的机制，属于下半部。**



### 计算机操作系统-死锁

##### 1.死锁产生的必要条件是什么？

**死锁是两个或两个以上的进程(或线程)在执行过程中，互相占用对方所需的资源，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去；**

**互斥：**每个资源要么已经分配给了一个进程，要么就是可用的；

**占有和等待：**已经等到某个资源的进程可用再请求新的资源；

**不可抢夺：**已经分配给一个进程的资源不能强制性的被抢夺，它只能被占有它的线程显式地释放；

**环路等待：**有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源；

##### 2.详解银行家算法

**银行家算法是一种可以避免死锁的方法。**

> 以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。我们可以把操作系统看做是银行家，操作系统管理的资源相当与银行家管理的资金，进程向操作系统请求分配资源就相当于用户向银行家贷款。

**实现方法：**

为保证资金的安全，银行家规定:

(1) 当一个顾客对资金的最大需求量不超过银行家现有的资金时就可接纳该顾客;
**（即当资源池中剩余的可利用资源 >= 线程还需要的资源时，就可以将可利用资源分配给此线程）**
(2) 顾客可以分期贷款,但贷款的总数不能超过最大需求量;
**（线程可以请求分配资源，但是请求的资源总数不能超过资源池中剩余的可利用资源）**
(3) 当银行家现有的资金不能满足顾客尚需的贷款数额时,对顾客的贷款可推迟支付,但总能使顾客在有限的时间里得到贷款;
**（当线程池中的资源暂时不满足当前的线程所需时，将此线程先暂时搁置，先将资源分配给能够满足的需求的其他线程，等到线程池中的资源足够满足先前搁置的线程时，在将资源分配给搁置的线程）**
(4) 当顾客得到所需的全部资金后,一定能在有限的时间里归还所有的资金。
**（当线程拿到所需要的所有资源，运行结束后，将自身所有的资源放回资源池中）**





##### 2.死锁的解决策略

鸵鸟不做处理；

死锁检测与恢复；

死锁预防；

死锁避免；



主要有以下四种办法：

- **鸵鸟策略**

把头埋在沙子里面，假装根本没发生问题。当发生死锁时不会对用户造成很大的影响或发生死锁的概率很低时，可以采用鸵鸟策略；

- **死锁检测与死锁恢复；**

不试图阻止死锁，而是当**检测**到死锁时，采取措施进行**恢复**；

死锁的检测是通过**检测有向图是否存在环来实现的**。从**一个结点出发进行深度优先搜索，对已访问过的结点进行标记，如果访问了已标记的结点，就表示有向图存在环，也就是检测到死锁的发生**。

死锁的恢复：通过**杀死线程恢复**；通过回滚恢复；通过抢占恢复；

- **死锁预防；**

针对死锁的产生条件：

**破坏互斥条件：**例如打印机唯一真正请求物理打印机的进程是打印机守护进程；

**破坏占有和等待条件：**规定进程在开始执行前请求所需要的全部资源；

**破坏不可抢夺条件**

**破坏环路等待：**给资源统一编号，进程只能按照编号顺序来请求资源；



**死锁避免：安全状态。**



### 计算机操作系统-进程管理

#### 0.进程调度  进程通信  进程同步

##### 0.1 进程调度算法

**1.等待态：等待某个事件的完成；**

**2.就绪态：等待系统分配处理器以便运行；**

**3.运行态：占有处理器正在运行。**



**先来先服务算法；**

　　先来先去服务调度算法是一种最简单的调度算法，也称为先进先出或严格排队方案。当每个进程就绪后，它加入就绪队列。当前正运行的进程停止执行，选择在就绪队列中存在时间最长的进程运行。该算法既可以用于作业调度，也可以用于进程调度。先来先去服务比较适合于常作业（进程），而不利于段作业（进程）。

**时间片轮转算法；**

　　轮转法是基于适中的抢占策略的，以一个周期性间隔产生时钟中断，当中断发生后，当前正在运行的进程被置于就绪队列中，然后基于先来先去服务策略选择下一个就绪作业的运行。这种技术也称为时间片，因为每个进程再被抢占之前都给定一片时间。

**最短进程优先**；

　　最短进程优先是一个非抢占策略，他的原则是下一次选择预计处理时间最短的进程，因此短进程将会越过长作业，跳至队列头。该算法即可用于作业调度，也可用于进程调度。但是他对长作业不利，不能保证紧迫性作业（进程）被及时处理，作业的长短只是被估算出来的。

**最短剩余时间优先；**

　　**最短剩余时间是针对最短进程优先增加了抢占机制的版本。在这种情况下，进程调度总是选择预期剩余时间最短的进程**。当一个进程加入到就绪队列时，他可能比当前运行的进程具有更短的剩余时间，因此只要新进程就绪，调度程序就能可能抢占当前正在运行的进程。像最短进程优先一样，调度程序正在执行选择函数是必须有关于处理时间的估计，并且存在长进程饥饿的危险。

**最高响应比优先；**

根据比率：R=(w+s)/s （R为响应比，w为等待处理的时间，s为预计的服务时间）

　　如果该进程被立即调用，则R值等于归一化周转时间（周转时间和服务时间的比率）。R最小值为1.0，只有第一个进入系统的进程才能达到该值。调度规则为：当前进程完成或被阻塞时，选择R值最大的就绪进程，它说明了进程的年龄。当偏向短作业时，长进程由于得不到服务，等待时间不断增加，从而增加比值，最终在竞争中赢了短进程。

　　和最短进程优先、最短剩余时间优先一样，使用最高响应比策略需要估计预计服务时间。

**反馈队列算法；**

**如果没有关于进程相对长度的任何信息，则最短进程优先，最短剩余时间、最高响应优先比都不能使用**。另一种导致偏向短作业的方法是**处罚运行时间较长的作业，换句话说，如果不能获得剩余的执行时间，那就关注已执行了的时间。**

　　方法为：调度基于被抢占原则（按时间片）并使用**动态优先级机制。**当一个进程第一次进入系统中时，他被放置在一个优先级队列中，当第一次被抢占后并返回就绪状态时，它被放置在**下一个低优先级队列中，在随后的时间里，每当被抢占时，他被降级到下一个低优先级队列中。**一个短进程很快被执行完，不会在就绪队列中降很多级，**一个长进程会逐渐降级。因此先到的进程和短进程优先于长进程和老进程。**在每个队列中，除了优先级在最低的队列中之外，都是用简单的先来先去服务机制，一旦一个进程处于优先级最低的队列中，它就不可能在降级，但会重复的返回该队列，直到运行结束。因此，该队列课按照轮转方式调度。

**多级反馈队列调度算法；**

**多级反馈队列算法，不必事先知道各种进程所需要执行的时间，他是当前被公认的一种较好的进程调度算法。其实施过程如下：**

　　1)**设置多个就绪队列，并为各个队列赋予不同的优先级。在优先权越高的队列中，为每个进程所规定的执行时间片就越小。**

　　2)当一个新进程进入内存后，首先放入第一队列的末尾，按照**先来先去原则排队等候调度。如果他能在一个时间片中完成，便可撤离；**如果未完成，就转入第二队列的末尾，同样等待调度.....如此下去，当一个长作业（进程）从第一队列依次将到第n队列（最后队列）后，便按第n队列时间片轮转运行。

　　3)仅当第一队列空闲的时候，调度程序才调度第二队列中的进程运行；仅当第1到（i-1）队列空时，才会调度第i队列中的进程运行，并执行相应的时间片轮转。

　　4)如果处理机正在处理第i队列中某进程，又有新进程进入优先权较高的队列，则此新队列抢占正在运行的处理机，并把正在运行的进程放在第i队列的队尾。

##### mmap

mmap是一种内存映射文件的方法，即将**一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。**相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示：

![image-20210729133222808](imgs\548.png)

由上图可以看出，进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为**内存映射服务的地址空间**处在**堆****栈**之间的空余部分。



##### 0.2 进程间8种通信方式详解

**进程通信**

每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，即**在内核中开辟一块缓冲区，进程A把数据从用户空间拷贝到内核缓冲区，进程B再从内核缓冲区把数据读走，内核提供的这种方式就被成为进程间通信。**

###### a.匿名管道通信

**匿名管道( pipe )：**管道是一种半双工的通信方式，数据只能**单向流动**，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指**父子进程关系**。

```
// 需要的头文件
#include <unistd.h>

// 通过pipe()函数来创建匿名管道
// 返回值：成功返回0，失败返回-1
// fd参数返回两个文件描述符
// fd[0]指向管道的读端，fd[1]指向管道的写端
// fd[1]的输出是fd[0]的输入。
int pipe (int fd[2]);
```

通过匿名管道实现进程间通信的步骤如下：

- 父进程创建管道，得到两个⽂件描述符指向管道的两端；
- 父进程fork出子进程，⼦进程也有两个⽂件描述符指向同⼀管道。
- 父进程关闭fd[0],子进程关闭fd[1]，即⽗进程关闭管道读端,⼦进程关闭管道写端（因为管道只支持单向通信）。⽗进程可以往管道⾥写,⼦进程可以从管道⾥读,管道是⽤环形队列实现的,数据从写端流⼊从读端流出,这样就实现了进程间通信。

###### b.高级管道通信

高级管道(popen)：将**另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程**，这种方式我们成为高级管道方式。

###### c.有名管道通信

有名管道 (named pipe) ： **有名管道也是半双工的通信方式**，但是**它允许无亲缘关系进程间的通信。**

###### d.消息队列通信

消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。**消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点**。

//创建和访问一个消息队列
int msgget(key_t key,int msgflag);
//消息队列的控制函数
int msgctl(int msqid,int cmd,struct msqid_ds *buf);
//把一条消息添加到消息队列中
int msgsnd(int msqid,const void *msgp,size_t msgsz,int msgflg);
//从一个消息队列中接收消息

- 消息队列可以双向通信

* 克服管道只能横在无格式字节流的缺点

###### e.信号量

信号量( semophore ) ： **信号量是一个计数器，可以用来控制多个进程对共享资源的访问。**它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

###### f.信号

信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。

###### g.共享存储

共享内存( shared memory ) ：**共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，**但多个进程都可以访问。**共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。**



共享内存原理每个进程都有自己的进程控制块PCB和地址空间Addr space，并且都有一个与之对应的页表，负责将进程之间的虚拟地址与物理地址进行映射，通过内存管理单元进行管理，**两个不同的进程的虚拟地址通过页表映射到物理区域的同一区域，它们所指向的内存称为共享内存**

![image-20210729120321751](imgs\547.png)

###### h.套接字通信

socket bind listen connect read write close

##### 0.3 进程 同步详解





#### 0.Windows下CPU核数和线程数？CPU核数和线程的关系

**CPU单核和多核的区别？**

最早的CPU都是单核，单核cpu也就是单核处理器，后来演变出了多核CPU，多核心cpu把两个或更多独立处理器封装在一个单一集成电路，每个核心分别负责处理一项运算，而单核CPU在多个应用的相互切换中就显得略有不足，比不上多核了。cpu多核的执行速度较单核更快，不容易造成卡顿，更流畅。



**CPU个数**即CPU芯片个数

**CPU的核心数**是指物理上，也就是硬件上存在着几个核心。比如，双核就是包括2个相对独立的CPU核心单元组，四核就包含4个相对独立的CPU核心单元组。

**线程数**是一种逻辑的概念，简单地说，就是模拟出的CPU核心数。比如，可以通过一个CPU核心数模拟出2线程的CPU，也就是说，这个单核心的CPU被模拟成了一个类似双核心CPU的功能。我们从任务管理器的性能标签页中看到的是两个CPU。

CPU的线程数概念仅仅只针对Intel的CPU才有用，因为它是通过Intel超线程技术来实现的，最早应用在Pentium4上。如果没有超线程技术，一个CPU核心对应一个线程。

CPU之所以要增加线程数，是源于多任务处理的需要。线程数越多，越有利于同时运行多个程序，因为**线程数等同于在某个瞬间CPU能同时并行处理的任务数。 因此，线程数是一种逻辑的概念，简单地说，就是模拟出的 CPU 核心数**。一个核心最少对应一个线程，但英特尔有个超线程技术可以把一个物理线程模拟出两个线程来用，充分发挥 CPU 性能，即一个核心可以有两个到多个线程。



**CPU核数和线程数有什么用？**

多核心和多线程的作用主要是为了满足各类程序多任务需求，核心数和线程数量越多，那么越有利同时运行多个程序，CPU能够并行处理多个任务数量，说白点就是核心数和线程数量越多，越适合多开软件或者游戏，打开的任务越多，除了多开程序，还有渲染需求，核数和线程数越多，越有利。





**那计算机只有一个cpu为什么能同时运行多个软件？**

**中央处理器（CPU），是电子计算机中的核心配件,相当于人的大脑，人的大脑可以控制人的身体，而计算机的大脑CPU可以控制计算机**。**我们打开任务管理器，就会发现其实一个系统中运行了多个程序，这就给我们造成了一个假象，就是我们电脑可以同时运行很多很多程序，其实不是的。**在某一时刻，只有一个程序在运行！我们的电脑运行多个程序，但是他们并不是并行的，而是**并发**的。也就是**他们并非在同一时刻同时运行，而是在快速的切换，在共享CPU这个资源而已。**

**正在运行的程序其实随着时间的前进，进程在CPU上面在不断地切换，因为我们的CPU的性能实在是太强了，处理的非常迅速，以至于我们根本感觉不到卡顿**。比如我们在操作QQ，那么CPU就将很多时间分配给QQ，将极少的资源分配给别的进程。当我们使用的应用变为别的时，那CPU又开始重点处理别的进程了。

#### 0.有了进程为什么还要线程呢？

**进程有很多优点**，它提供了多道编程，让我们感觉我们**每个人都拥有自己的CPU和其他资源，可以提高计算机的利用率。**很多人就不理解了，既然进程这么优秀，为什么还要线程呢？其实，仔细观察就会发现**进程还是有很多缺陷的**，主要**体现在两点**上：

- **进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了**。
- 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。

举个现实的例子也许你就清楚了：**如果把我们上课的过程看成一个进程的话，那么我们要做的是耳朵听老师讲课，手上还 要记笔记，脑子还要思考问题**，这样才能高效的完成听课的任务。而如果只提供进程这个机制的话，上面这三件事将不能同时执行，**同一时间只能做一件事**，听的时 候就不能记笔记，也不能用脑子思考，这是其一；如果老师在黑板上写演算过程，我们开始记笔记，而老师突然有一步推不下去了，阻塞住了，他在那边思考着，而 我们呢，也不能干其他事，即使你想趁此时思考一下刚才没听懂的一个问题都不行，这是其二。

现在你应该明白了进程的缺陷了，而解决的办法很简单，我们**完全可以让听、写、思三个独立的过程，并行起来，这样很明显可以提高听课的效率**。而实际的操作系统中，也同样引入了**这种类似的机制——线程。**

**线程**

进程和线程的并发层次不同：**进程属于在处理器这一层上提供的抽象**；**线程则属于在进程这个层 次上再提供了一层并发的抽象**。



线程还有个好处，就是可以有效地利用多处理器和多核计算机。现在的处理器有个趋势就是朝着多核方向发展，**在没有线程之前，多核并不能让一个进程的执行速度提高，原因还是上面所有的两点限制**。但如果讲一个进程分解为若干个线程，则**可以让不同的线程运行在不同的核上，从而提高了进程的执行速度。**



#### 1.Linux下多线程和多进程程序的优缺点

**进程的创建对于不同的操作系统是不同的：**

1. 对于 Windows 系统来说，进程创建开销很大，因此 Windows 多线程学习重点是要大量面对资源争抢与同步方面的问题。
2. 对于 Linux 系统来说，进程创建开销很小，因此，Linux 下的学习重点是，大家要学习进程间通讯的方法。





多线程的优点：

- **方便高效的内存共享**：多进程下内存共享比较不便，**且会抵消多进程编程的好处。**
- **较轻的上下文切换开销**：不用切换地址空间，不用更改CR3寄存器，不用清空TLB。

多进程的优点：

- **更强的容错性**：一个进程crask不会导致整个系统崩溃；
- **更好的多核可伸缩性**：进程的使用将许多内核资源（如地址空间，页表，打开的文件）隔离，在多核系统上的可伸缩性强于多线程程序；

综上，**当你的不同任务间需要大量共享数据或频繁通信时**，使用多线程，其它情况下尽量使用多进程。



多进程的优点是**稳定性好**，一个子进程崩溃了，不会影响主进程以及其余进程。基于这个特性，常常会用多进程来实现守护服务器的功能。多进程编程也有不足，即**创建进程的代价非常大，因为操作系统要给每个进程分配固定的资源**，并且操作系统对进程的总数会有一定的限制，若进程过多，操作系统调度都会存在问题，会造成假死状态。

多线程编程的优点是**效率较高一些，适用于批处理任务等功能**；不足之处在于，**任何一个线程崩溃都可能造成整个进程的崩溃，因为它们共享了进程的内存资源池。**



既然**多线程编程和多进程编程**各有优缺点，因此它们分别适用于不同的场景。比如说，**对于计算密集型的任务，多进程效率会更高一下**；而**对于IO密集型的任务（比如文件操作，网络爬虫），采用多线程编程效率更高**。为什么是这样呢？



最好是多进程和多线程结合，即根据实际的需要，每个CPU开启一个子进程，这个子进程开启多线程可以为若干同类型的数据进行处理。







#### 1.进程与线程的区别是什么(高频高频)？

在 Java1.2 之后. Linux中的JVM是基于`pthread`实现的,即 **现在的Java中线程的本质，其实就是操作系统中的线程**

**a.拥有资源**

**进程是资源分配的基本单位**，**但是线程不拥有资源**，线程可以访问隶属进程的资源。

**b.调度**

线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程的切换，从一个进程的线程切换到另一个进程中线程时，会引起进程切换。

**c.系统开销**

由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O设备，**所付出的开销远大于创建或撤销线程时的开销**。 在进程进程切换时，涉及当前执行进程CPU环境的保存以及新调度进程CPU环境的设置，而线程切换时只需保存和设置少量寄存器内存，开销很小；

**d.通信方法**

线程间**可以通过直接读写同一进程中的数据**进行通信，但是**进程间通信需要借助IPC**。





**a.最大区别：**

- <u>进程是系统资源分配的最小单位；</u>
- <u>线程是cpu操作和调度的最小单位；</u>

**b.从创建成本、切换以及通信成本来说:**

**线程的创建成本远远低于进程：**

- 创建进程需要为进程划分出一块完整的内存空间，有大量的初始化操作，比如要把内存分段(**代码段、数据段、堆区、文件映射区、栈区和内核态**；)
- 创建线程则简单的多，只需要确定**PC指针**和**寄存器**的值，并且给线程分配一个**栈**用于执行程序，同一个进程的多个线程间可以复用**堆栈**；
- 因此，创建进程比创建线程慢，而且进程的内存开销更大。
- ![image-20210607195339887](imgs\300.png)

**上下文切换**

- 进程上下文切换

当一个进程 在执行时，<u>**CPU的所有寄存器中的值、进程的状态以及堆栈中的内容**</u>被称为该进程的上下文。

<u>当内核需要切换到另一个进程时，它就需要保存当前进程的所有状态，即保存当前进程的上下文，以便在再次执行该进程时，能够必得到切换时的状态执行下去。</u>

- 线程上下文切换

当进程只有一个线程时，可以认为进程就等于线程；

当进程拥有多个线程时，这些**线程共享相同的虚拟内存和全局变量等资源**，这些资源在上下文切换时并不需要修改。

那么根据上面的描述，<u>线程的上下文切换就存在两种情况：</u>

- <u>前后切换的两个线程是属于同一个进程</u>，两个线程的资源基本是共享的，<u>切换上下文共享的资源无需变动，**只有当线程有私有数据时，切换这些不共享的数据即可。**</u>
- <u>前后切换的两个进程不属于同一个进程，跟切换进程的上下文时是一样的</u>；

所以切换同一进程的线程比切换进程消耗更少的系统资源，这就是多线程比多进程的优势。

**通信方法**

线程间**可以通过直接读写同一进程中的数据**进行通信，但是**进程间通信需要借助IPC**。



#### 1.线程和协程的区别是什么？

**什么是进程，什么是线程？**

进程就是应用程序的启动实例。比如我们运行一个游戏，打开一个软件，就是开启了一个进程。

线程从属于进程，是程序的实际执行者。一个进程至少包含一个主线程，也可以有更多的子线程。

**进程是资源分配的基本单位，线程是操作系统调度的基本单位。**



而无论是进程还是线程，都是由操作系统所管理的，其Java中线程具有多种状态。而线程不同状态之间的转换是由谁来实现的呢？JVM通过操作系统内核中的TCB(Thread Control Block)模块来改变线程的状态，这一过程需要耗费一定的CPU资源。



**b.进程和线程的痛点**

线程之间是如何进行协作的呢？

最经典的例子就是生产者/消费者模式。若干个生产者线程向队列中写入数据，若干个消费者线程从队列中消费数据。

但上面虽然正确实现了生产者/消费者模式，但并不是一个高性能的实现。为什么性能不高呢？

- **涉及到同步锁**；
- 涉及到线程**阻塞状态和可运行状态之间的切换**；
- 涉及线程的**上下文切换；**

以上涉及到的任何一点，都是非常耗费性能的操作的。



**c.什么是协程呢**

协程，是一种比线程更加轻量级的存在。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。

最重要的是，**协程不是被操作系统内核所管理**，而**完全是由程序所控制**（也就是在用户态中执行）。

这样带来的好处就是性能得到了很大的提升，不会**像线程切换那样消耗资源**。

既然协程这么好，它到底是怎么来使用的呢？

由于Java的原生语法中并没有实现协程（某些开源框架实现了协程，但是很少被使用），所以我们来看一看python当中对协程的实现案例，同样以生产者消费者模式为例：

![image-20210622103540270](imgs\348.png)

其中 **yield** 是python当中的语法。当协程执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。

但是，yield让协程暂停，和线程的阻塞是有本质区别的。协程的暂停完全由程序控制，**线程的阻塞状态是由操作系统内核来进行切换。**

因此，**协程的开销远远小于线程的开销。**



![image-20210507151659609](E:/笔记/面试高频/imgs/196.png)

**协程是一种用户态的轻量级线程。**协程的调度完全由用户控制，**协程拥有自己的寄存器上下文和栈**。协程调度切换时，将寄存器上下文和栈保存到其它地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有**内核切换的开销**，可以不加锁的访问全局变量，所以上下文的切换非常快。



**进程与协程的比较**

- 一个线程可以有多个协程，一个进程也可以单独拥有多个协程；
- **线程进程都是同步机制，而协程则是异步机制；**
- **协程能保留上一次调用的状态，每次重入时，就相当于进入上一次调用的状态；**

### 2.Linux的fork和vfork函数

对于fork函数：

子进程只继承父进程的文件描述表，不继承但共享文件表项和i-node

父进程创建一个子进程之后，文件表项中的引用计数加１变为２，**当父进程作close操作之后计数器减１，子进程还是可以使用文件表项，只有计数器减到０的时候才会释放该文件表项。**

![image-20210818125823691](imgs\711.png)

fork（）与vfock（）都是创建一个进程，那他们有什么区别呢？总结有以下三点区别：

1. fork （）：子进程拷贝父进程的数据段，代码段
     vfork （ ）：子进程与父进程共享数据段
2. fork （）父子进程的执行次序不确定
     vfork 保证子进程先运行，在调用exec 或exit 之前与父进程数据是共享的,在它调用exec或exit 之后父进程才可能被调度运行。
3. vfork （）保证子进程先运行，在她调用exec 或exit 之后父进程才可能被调度运行。如果在
     调用这两个函数之前子进程依赖于父进程的进一步动作，则会导致死锁。

#### 2.CPU的下文切换（进程上下文切换  线程上下文切换以及中断的上下文切换）

进程在竞争 CPU 的时候并没有真正运行，为什么还会导致系统的负载

升高呢？CPU 上下文切换就是罪魁祸首。

我们都知道，Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。

而在每个**任务运行前**，**CPU 都需要知道任务从哪里加载、又从哪里开始运行**，也就是说，需要系统事先帮它设置好 **CPU 寄存器和程序计数器**（Program Counter，PC）。

CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而**程序计数器，则是用来存储CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境**，因此也被叫做 **CPU 上下文**。

![image-20210630190308978](imgs\377.png)

知道了什么是 CPU 上下文，我想你也很容易理解 **CPU 上下文切换**。CPU 上下文切换，就是先把**前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来**，**然后加载新任务的上下文**到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，**运行新任务**。

而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

操作系统管理的这些“任务”到底是什么呢？

所以，根据任务的不同，**CPU 的上下文切换**就可以分为几个不同的场景，也就是**进程上下文切换**、**线程上下文切换**以及**中断上下文切换**。



**进程上下文切换**

Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中，CPU 特权等级的 Ring 0 和 Ring 3。

内核空间（Ring 0）具有最高权限，可以直接访问所有资源；

用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。

![image-20210630190543636](imgs\378.png)

那么，系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。

CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。

而系统调用结束后，CPU 寄存器需要**恢复**原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，**一次系统调用的过程，其实是发生了两次 CPU 上下文切换。**

**进程上下文切换，是指从一个进程切换到另一个进程运行。**

**而系统调用过程中一直是同一个进程在运行。**



那么，进程上下文切换跟系统调用又有什么区别呢？

**进程是由内核来管理和调度的**，进程的切换只能发生在内核态。所以，进程的上下文不仅包括了**虚拟内存、栈、全局变量**等用户空间的资源，还**包括了内核堆栈、寄存器等内核空间的状态**。

因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，**需要先把该进程的虚拟内存、栈等保存下来**；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。



**那么，进程在什么时候才会被调度到 CPU 上运行呢？**

最容易想到的一个时机，就是进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行。其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下。

**其一**，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，**当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。**

**其二**，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。

**其三**，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。

**其四**，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。

**最后一个**，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。



**线程上下文切换**

线程与进程最大的区别在于，**线程是调度的基本单位，而进程则是资源拥有的基本单位**。

说白了，所谓**内核中的任务调度，实际上的调度对象是线程**；而进程只是给线程提供了**虚拟内存、全局变量**等资源。

这么一来，线程的上下文切换其实就可以分为两种情况：

第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。

到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。

**中断上下文切换**

除了前面两种上下文切换，还有一个场景也会切换 CPU 上下文，那就是中断。

为了快速响应硬件的事件，**中断处理会打断进程的正常调度和执行**，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。

跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。

**对同一个 CPU 来说，中断处理比进程拥有更高的优先级**，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。

#### 2.软中断

**从一个例子来理解中断**

中断是系统用来响应硬件设备请求的一种机制，它会打断进程的正常调度和执行，然后调用内核中的中断处理程序来响应设备的请求。

为什么要有中断呢？我可以举个生活中的例子，让你感受一下中断的魅力。

比如说你订了一份外卖，但是不确定外卖什么时候送到，也没有别的方法了解外卖的进度，但是，配送员送外卖是不等人的，到了你这儿没人取的话，就直接走人了。所以你只能苦苦等着，时不时去门口看看外卖送到没，而不能干其他事情。

不过呢，如果在订外卖的时候，你就跟配送员约定好，让他送到后给你打个电话，那你就不用苦苦等待了，就可以去忙别的事情，直到电话一响，接电话、取外卖就可以了。

这里的“打电话”，其实就是一个中断。**没接到电话的时候，你可以做其他的事情；只有接到了电话（也就是发生中断），你才要进行另一个动作：取外卖。**

这个例子你就可以发现，**中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力**。

由于中断处理程序会打断其他进程的运行，所以，**为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行**。如果中断本身要做的事情不多，那么处理起来也不会有太大问题；但如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间。



特别是，中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失。



那么还是以取外卖为例。假如你订了 2 份外卖，一份主食和一份饮料，并且是由 2 个不同的配送员来配送。这次你不用时时等待着，两份外卖都约定了电话取外卖的方式。但是，问题又来了。

当第一份外卖送到时，配送员给你打了个长长的电话，商量发票的处理方式。与此同时，第二个配送员也到了，也想给你打电话。

网卡接收到数据包后，会通过**硬件中断**的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。你可以自己先想一下，这种情况下的上半部和下半部分别负责什么工作呢？

**对上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态（表示数据已经读好了**），最后再发送一个**软中断**信号，通知下半部做进一步的处理。

而下半部被软中断信号唤醒后，需要从**内存中找到网络数据，再按照网络协议栈，对数据进行逐层解析和处理，直到把它送给应用程序。**

上半部直接处理**硬件请求，也就是我们常说的硬中断，特点是快速执行；**

而下半部则是由**内核触发，也就是我们常说的软中断，特点是延迟执行。**









#### 2.进程状态的切换（高频）

**进程的三态模型**

按进程在执行过程中的不同情况至少要定义三种状态：

- **运行（running）态**：进程占有处理器正在运行的状态。

  > 进程已获得CPU，其程序正在执行。在单处理机系统中，只有一个进程处于执行状态； 在多处理机系统中，则有多个进程处于执行状态。

- **就绪（ready）态**：进程具备运行条件，等待系统分配处理器以便运行的状态。

  > 当进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。

- **等待（wait）态**：又称阻塞态或睡眠态，指进程不具备运行条件，正在等待某个时间完成的状态。

  > 也称为等待或睡眠状态，一个进程正在等待某一事件发生（例如请求I/O而等待I/O完成等）而暂时停止运行，这时即使把处理机分配给进程也无法运行，故称该进程处于阻塞状态。

![image-20210712222456476](imgs\452.png)

**就绪->运行态**：CPU 空闲时选择一个就绪进程。

**运行->就绪**：时间片轮转使得每一个片时间来获得cpu运行，当时间片到时会从运行状态到就绪状态；或者有更高优先级的抢占cpu；

**运行态到等待态**：等待使用资源；如等待外设传输；等待人工干预。

**等待态到就绪态**：资源得到满足；如外设传输结束；人工干预完成。







使用ps或者top可查看进行的状态。这些状态包括运行、空闲、不可中断睡眠、可中断睡眠、僵尸以及暂停等。其中，我们重点学习了不可中断状态和僵尸进程：

- 不可中断状态，一般表示进程正在跟硬件交互，为了保护进程数据与硬件一致，系统不允许其他线程或中断该进程。
- 僵尸进程：表示进程已经退出，但它的父进程没有回收该进程所占用的资源。



**就绪状态**(ready)：**等待被调度**；

**运行状态**(running)

**阻塞/等待状态**(waiting)：**等待资源**



> 笔试题：进程进入等待状态有哪几种方式：获得splinlock为果。
>
> 解题思路：进程分为基本的三个状态：运行、就绪、阻塞/等待。
>
> A.**高优先级的抢占CPU**，使得原来处于**运行状态**的进程转变为**就绪状态**；
>
> B.**阻塞的进程等待某件事情的发生**，一旦发生则它的运行条件已经满足，从**阻塞**进入**就绪状态；**
>
> C**.时间片轮转使得每个进程都有一小片时间来获得CPU运行，** 当时间片到时会从**运行状态**变为**就绪状态**。
>
> D.自旋锁是一种保护临界区最常见的技术，在同一时刻只能有一个进程获得自旋锁，其它企图没有获得自旋锁的任何进程将一直进行尝试，除此以外不能做任何事情。因此**没有获得自旋锁的进程在获取锁之前处于阻塞状态**。







#### 3.进程调度算法

> 适用于早起批处理系统的调度算法
>
> - 先来先服务(FCFS);
> - 短作业优先(SJF);
> - 高响应比优先(HRRN)

不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法；

**批处理系统：**

- **先来先服务**(按照请求顺序)；
- **短作业优先(**按照运行时间最短的顺序调度)；
- **最短剩余时间优先(**按照剩余运行时间调度顺序调度)；

**交互式系统：**

- **时间片轮转；**
- **优先级调度；**



#### 3.进程调度算法

**先来先服务(FCFS)**是最简单调度算法。按照进入后备队列的先后次序来加入就绪队列等待执行。是非抢占式，易于实现，效率不高，利用长作业(CPU繁忙)，不利于短作业(I/O繁忙)。

**短作业优先**：是非抢占式的，具有很好性能，降低平均等待时间，提高吞吐量。不利于厂作业，可能一直处于等待出现饥饿，未考虑作业紧迫程度，不能用于实时系统；

**高响应比优先调度算法**：（响应比高意味着等待时间长而服务时间短，优先处理）响应比=（等待+服务）/ 服务时间 = 等待/服务时间 + 1;

**时间片轮转**：用于分时系统的进程调度，抢占式；

**多级反馈队列调度算法**：将时间轮转和优先级调度结合，把进程按优先级分成不同的队列，先按优先级调度，优先级相同，按时间片轮转。优点是兼顾长短作业，有较好的响应时间，可行性强，适用于各种作业。



#### 3.抢占式调度算法和非抢占式的调度算法

非抢占式（Nonpreemptive）   让进程运行直到结束或阻塞的调度方式   容易实现   适合专用系统，不适合通用系统   
抢占式（Preemptive）   允许将逻辑上可继续运行的在运行过程暂停的调度方式   可防止单一进程长时间独占CPU   系统开销大（降低途径：硬件实现进程切换，或扩充主存以贮存大部分程序）









#### 4.进程同步与进程通信

- 进程同步：控制多个进程按一定顺序执行；
- 进程通讯：进程间传输信息；

#### 5.进程同步

- **临界区：** 对临界资源进行访问的那段代码被称为临界区；
- **同步与互斥：** 同步多个进程有一定的先后执行关系；互斥是多个进程在同一时刻只能有一个进程能进入临界区；
- **信号量：** 信号量是一个整型变量，可以对其执行down和up操作，即常见的P和V操作；
- **管程：**使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来了。

> 哲学家就餐的问题，如何防止死锁的发生，可以设置两个条件：
>
> - 必须同时拿起左右两根筷子；
> - 只有在两个邻居都没有进餐的情况下才允许就餐；



**共享内存的生命周期是随内核的。
匿名管道、命名管道、消息队列、信号量、共享内存这五种进程间通信方式。两种管道的生命周期是随进程，剩下的都是随内核的。**

#### 6.进程通信（高频）

进程同步与进程通信很容易混淆，它们的区别在于：

- 进程同步：**控制多个进程按一定顺序执行；**
- 进程通信：**进程间传输信息；**

<u>为了能够达到进程同步的目的，需要让进程间进行通信，传输一些进程同步所需要的信息；</u>

**a.管道：用于兄弟进程或父子进程的通信**

管道是通过**调用pipe函数创建的**，**fd[0]用于读，fd[1]用于写**；

- 只支持**半双工通信(单向交替传输)**；
- 只能在**父子进程或者兄弟进程中使用**；

**b.FIFO命名管道：去除了管道只能父子进程通信的限制**

也被称为命名管道，**去除了管道只能在父子进程中使用的限制**。常用于客户进程和服务器进程之间传递数据。

**c.消息队列：避免了FIFO的同步阻塞问题**

相比较FIFO，消息队列：

- 可以独立于读写进程存在；
- 避免了**FIFO的同步阻塞问题**；
- 读进程可以根据消息类型有选择地接收消息，而不像FIFO那样只能默认地接收。

**d.信号量：用于多线程对共享数据对象的访问**

它是**一个计数器，用于为多个进程提供对共享数据对象的访问**；

**e.共享存储：允许多个进程共享一个给定的存储区**

**允许多个进程共享一个给定的存储区**，因为数据不需要在进程之间复制，所以这是最快的一种IPC；

**f.套接字：用于不同机器间的进程通信**

与其它通信机制不同，**它可用于不同机器间的进程通信**；



> 相关问题对比：
>
> 进程通信：管道、FIFO、消息队列、共享存储、信号量、套接字



> **那么线程间的同步通信方式有哪些呢？**
>
> 线程同步是两个或多个共享关键资源的线程的并发执行。
>
> - **互斥量：**保证公共资源不会被多个线程同时访问。比如Java中的synchronized关键字和各种Lock都是这种机制；
> - **信号量：**允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量；
> - **事件:** wait/notfiy 通过通知操作的方式来保持多线程同步，



#### 7.什么是线程安全呢？

刚刚提到了进程和线程的区别，**进程是资源分配的基本单位，多个线程共用进程中的资源。**

<u>**目前主流操作系统为了保证安全，每个进程只能访问分配给自己的内存空间，而不能访问别的进程的**</u>，而在每个进程的内存空间中都有一块特殊的公共区域，通常被称为堆(内存)。而进程内的所有线程都可以访问这块公共区域，而这就是造成问题的潜在原因。

如果多个进程只是读这块 公共区域，那么没什么问题。

但**是如果有一个进程，处理数据到一半，突然切换处理别的任务；而此时另一个线程将这个数据更改了，当第一个线程切换回来的时候发现数据已经不是刚才的样子，已经被修改了，存在意外修改的风险了。**

**<u>所谓线程安全指的就是，在堆内存中的数据由于可以被任何线程访问到，在没有限制的情况下存在被意外修改的风险。**</u>

#### 8.用户态和内核态

![image-20210413153829671](imgs\52.png)

从图中可以看出系统调用将linux整个体系分为用户态和内核态。

内核态是一种特殊的软件程序，**特殊在其是控制计算机的硬件资源，例如协调CPU资源，分配内存资源，并且提供稳定的环境供应用程序运行**；

用户态就是**提供应用程序运行的空间，为了使应用程序访问到内核管理的资源例如CPU，内存，I/O。** 

内核必须提供一组通用的访问接口，这些接口就叫系统调用。

**(1) 系统调用**

系统调用是操作系统的最小功能单位。系统调用组成了内核态和用户态交互的基本接口。

**(2)库函数**

库函数就是屏蔽这些复杂的底层实现细节，减轻程序员的负担，从而更加关注上层的逻辑实现。它对系统调用进行封装，提供简单的基本接口给用户。

**(3)shell**

shell就是外壳的意思，就像把内核包裹起来的外壳，是一种特殊的应用程序，俗称就是命令行。

为了方便用户和系统交互，一般一个shell对应一个终端，呈现给用户交互窗口。

**(4)用户态到内核态的切换**

其实用户态和内核态的含义，Linux设计的初衷就是给不同的操作给与不同的权限。Linux操作系统将权限分为了2个等级，分别就是内核态和用户态。

**<u>用户态和内核态不同之处在于它们权限不同。用户态的进程能够访问的资源受到了极大的限制，而运行在内核态的进程可以为所欲为。</u>**

一个进程可以运行在用户态也可以运行在内核态，那它们之间肯定存在用户态和内核态切花的过程。那么如何从用户态切换到内核态呢？

-  **发生系统调用时；**

这是出于用户态的进程主动请求切换到内核态的一种方式。<u>用户态的进程通过系统调用申请适用操作系统提供的系统调用服务例程来处理任务。</u>而系统调用的机制，其核心仍是使用了操作系统为用户特别开发的一个中断机制，即软中断。

- **产生异常时；**

当<u>CPU执行运行在用户态的程序时，发生了**某些事先不可知的异常**，这时候会触发由当前运行的进程切换到处理此异常的内核相关的程序中，也就是转到了内核态，如缺页异常。</u>

- **外设产生中断时；**

**<u>当外设设备完成用户请求操作后，会向CPU发生相应的中断信号**，CPU会暂停下一条要执行的指令而转取执行与中断信号对应的处理程序。</u>  如果先前执行的指令是用户态的执行，那么转换就涉及到了用户态到内核态。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作；

**（5）中断分类**

- 外设中断；
- 系统调用；
- 异常；



#### 9.进程切换何时发生呢？

进程切换一定发生在中断/异常/系统调用处理过程中，常见的有以下情况：

- 时间片**中断**、IO**中断**后 更改优先级进程；（导致被中断进程进入**就绪态**）；
- 阻塞式**系统调用**、**虚拟地址异常**；（导致被中断进程进入**等待态**）
- 终止用**系统调用**、**不能继续执行的异常**；（导致被中断进程进入**终止态**）

举例说明：

- **时钟中断：**操作系统确定当前正在运行的**进程的执行时间是否已经超过了最大允许时间段**，如果超过了，进程必须切换到就绪态，调度另一个进程；
- **I/O中断：** 操作系统确定**是否发生了I/O活动**。如果I/O活动是一个或多个进程正在等待的事件，操作系统就把所有相应的阻塞态转换到就绪态，操作系统必须决定继续执行当前处于运行态的进程，还是让具有高优先级的就绪态进程抢占这个进程。
- **虚拟地址异常（内存失效）**：处理器访问一个虚拟内存地址，且此地址单元不在内存中，操作系统必须从外存中把包含这个引用的 内存块(页或段)调入内存中。在发出调入内存块的I/O请求之后，操作系统可以会执行一个进程切换，以恢复另一个进程的执行，**发生内存失效的进程被置为阻塞态**，**当想要的块调入内存中时，该进程被置为就绪态**；
- 对于**陷阱**：操作系统确定错误或异常条件是否是致命的。如果是，当前正在运行的进程被转换到退出态，并发生进程切换；如果不是，操作系统 的动作取决于错误的种类 和操作系统的设计，其行为可以是试图恢复或通知用户，操作系统可能会进行一次进程切换或者继续执行当前正在运行的进程。
- 最后操作系统可能被来自正在执行的程序的系统调用激活。例如，一个用户进程正在运行，并且正在执行一条请求I/O操作的指令，如打开文件，这个调用导致转移到作为操作系统代码一部分的一个例程上进行。通常，使用系统调用会导致把用户 线程置为阻塞态；

#### 10.那么什么时候不能进行进程调度呢？

- **在处理中断的过程中；**
- **进程在操作系统内核程序临界区中；**
- **原子操作的过程中；**



#### 11.中断/异常一定会引起线程切换吗？

有一些中断/异常不会引起进程转换，只是**在处理完成后把控制权交还给了被中断进程。**

以下是处理流程：

- a. (中断/异常等触发)正向模式切换 并压入PSW/PC当中；
- b. 保存被中断进程的现场信息；
- c. 处理具体中断、异常；
- d. 恢复被中断进程调度现场信息；
- e. (中断返回指令触发)逆向模式转换并弹出PSW/PC；



### 计算机操作系统-内存管理

操作系统的内存管理主要负责**内存的分配与回收(malloc函数：申请内存；free函数：释放内存)，另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做得事情。**

**内存管理的方式有哪几种？**

- **块式管理：**相当古老的计算机操作系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费。这些在块中未被利用的空间，称之为碎片。
- **页式管理：**把主存分为大小相等且固定的一页一页的形式，页较小，相对于块式管理的划分粒度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址；
- **段式管理：**页式管理虽然提高了内存利用率，但是**页式管理其中的页实际上并无任何意义**。段式管理把主存分为一段段的，每一段的空间要比一页的空间小很多。但是，最重要的是**段有实际意义的，每个段定义了一组逻辑信息，例如有主程序段MAIN、子程序段X、数据段D以及栈段S。**段式管理通过段表对应逻辑地址和物理地址。
- **段页式管理：**段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又划分成若干页。段页式管理机制中段与段之间以及段的内部都是离散的。

#### 0.堆与栈的区别是什么(计算机操作系统、数据结构)

#####  (1) 计算机操作系统的栈

###### 1.1 栈的基本性质

**①内存管理：**

栈由编译器自动分配释放;

**②栈中的内容存放**
栈存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈<u>。满足：“先进后出”的原则存取</u>，也就是位于栈内的元素，必须等到其上面(对应的地址为较低的地址)的数据或函数执行完成后，弹出后才可以进行下面的元素的操作。

**③栈的结构：**

<u>栈的一端是固定的，另一端是浮动的。固定的一端是较高的地址，我们称栈顶。浮动的一端是较低的地址（最低到0）随着元素或方法的加入栈顶指针会逐步地往下移动</u>。所有的数据存入或取出，只能在浮动的一端（称栈顶）进行。（栈的生长方向是向下的，是向着内存地址减小的方向增长）

**④栈的速度：**
<u>栈是由系统自动分配的，一般速度较快</u>(栈的速度高于堆的速度)

**⑤申请大小的限制：**

栈是向低地址扩展的，是一块连续的内存的区域。是栈顶的地址和栈的最大容量是系统预先规定好的，栈的大小是2M（也有的说是1M，总之是一个编译时就确定的常数 ) ,如果申请的空间超过栈的剩余空间时，将提示overflow。因此，能从栈获得的空间较小。

###### 1.2 栈帧的概念

函数的每一次调用，均会在调用栈(call stack)上维护一个独立的栈帧(stack frame)。每个独立的栈帧一般包括：

- ①函数的返回地址和参数。
- ②临时变量: 包括函数的非静态局部变量以及编译器自动生成的其他临时变量。
- ③函数调用的上下文。

栈是从高地址向低地址延伸,一个函数的栈帧用ebp和esp这两个寄存器来划定范围.ebp 指向当前的栈帧的底部,esp 始终指向栈帧的顶部;
ebp 寄存器又被称为帧指针(Frame Pointer);
esp 寄存器又被称为栈指针(Stack Pointer);

![image-20210517160202238](imgs\244.png)



##### (2) 计算机操作系统中的堆

###### 2.1 堆的基本了解

**①堆的内存管理：**

堆是程序中一块预留的内存空间，可由程序自由使用，堆被程序申请使用的内存在被主动释放前一直有效。一般由程序员分配释放, 若程序员不释放,对于堆来讲，释放工作由程序员手动管理，不及时回收容易产生内存泄露。 程序结束时可能由操作系统回收。

**②C中对空间的申请：**C语言程序中通过头文件为“malloc.h”的库函数调用获得堆空间，关键字“malloc”以字节的方式动态申请堆空间，关键字“free”将堆空间归还给系统。

**③堆的速度：**有种说法是“栈是存放在一级缓存中的，而堆则是存放在二级缓存中的，<u>堆的生命周期由虚拟机的垃圾回收算法来决定（并不是一旦成为孤儿对象就能被回收）</u>。”所以调用这些对象的速度要相对来得低一些，故堆的速度慢于栈的速度。

**④系统对堆空间的管理方式：**我们了解下空闲链表法这种管理方式：：首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序，另外，对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。

##### (3) 计算机操作系统中堆与栈的对比

**①按分配方式分：**

栈：<u>栈有两种分配方式：静态分配和动态分配</u>

堆：<u>在C中堆是动态分配和回收内存的，没有静态分配的堆。</u>
（静态分配是系统编译器完成的，比如局部变量的分配.）
（动态分配是有malloc函数进行分配的，但是栈的动态分配和堆是不同的，它的动态分配也由系统编译器进行释放，不需要程序员手动管理）

**②申请大小的限制：**

栈：<u>栈是向低地址扩展的数据结构，是一块连续的内存的区域。</u>

堆：<u>堆是向高地址扩展的数据结构，是不连续的内存区域。</u>堆获得的空间比较灵活，也比较大。

**③效率：**

栈：由<u>系统自动分配，速度较快，不会产生内存碎片</u>。

堆：在C中堆是由malloc分配的内存，<u>速度比较慢，而且容易产生内存碎片，不过用起来较方便</u>。

使用堆时，频繁的用new/delete必会造成内存空间的不连续,导致造成大量碎片的产生,降低程序效率。而栈则不会有这个问题,因<u>为栈是先进后出的队列,它们是如此的一一对应,以至于永远都不可能有一个内存块从栈中间弹出。</u>

##### (4) 数据结构中的堆、栈

###### 4.1 栈

**①定义：** 栈是限制插入和删除只能在一个位置上进行的线性表。该位置即为表的末端也叫栈顶（top）栈又叫作“先进后出表”。

**②基本操作：** 栈的基本操作包括：push(进栈)和pop（出栈）。

**③栈的实现：** 栈是一个线性表，所以栈可以用数组或者链表来实现（即栈可以由逻辑上连续的ADT实现）。在java中大部分栈的实现是使用ArrayList或者LinkedList。

**④性能：**

索引：O(n)
查找：O(n)
插入：O(1)
删除：O(1)

![image-20210517161649463](imgs\245.png)

###### 4.2 堆

**①定义：** 堆是一种特别的树状数据结构。堆要满足以下特征：“给定堆中任意节点P和C，若P是C的母节点，那么P的值会小于等于（或大于等于）C的值”。

**逻辑定义**

![image-20210517162105991](imgs\246.png)

**堆是完全二叉树，完全二叉树不一定是堆**

**②堆的实现：** 

通过构造***二叉堆\***（binary heap），实为二叉树的一种；由于其应用的普遍性，当不加限定时，均指该数据结构的这种实现。这种数据结构具有以下性质：

- ①任意节点小于（或大于）它的所有后裔，最小元（或最大元）在堆的根上（堆序性）。
- ②堆总是一棵完全树。即除了最底层，其他层的节点都被元素填满，且最底层尽可能地从左到右填入。
- ③将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。常见的堆有二叉堆、斐波那契堆等。

<u>**最小堆：**若母节点的值恒小于等于子节点的值，此堆称为最小堆（min heap）。</u>

**<u>最大堆：**若母节点的值恒大于等于子节点的值，此堆称为最大堆（max heap）。</u>

在堆中最顶端的那一个节点，称作根节点（root node），根节点本身没有母节点（parent node）。

**③二叉堆的性能：**
索引：O(log(n))
查找：O(log(n))
插入：O(log(n))
删除：O(log(n))
删除最大值/最小值：O(1)

![image-20210517162316144](imgs\247.png)

**④应用：**堆排序、优先队列。

#### 0.什么是交换空间

操作系统把物理内存(physical RAM)分成一块一块的小内存，每一块内存被称为**页(page)**。当内存资源不足时，**Linux把某些页的内容转移至硬盘上的一块空间上，以释放内存空间**。硬盘上的那块空间叫做**交换空间**(swap space),而这一过程被称为交换(swapping)。**物理内存和交换空间的总容量就是虚拟内存的可用容量。**

用途：

- 物理内存不足时一些不常用的页可以被交换出去，腾给系统。
- 程序启动时很多内存页被用来初始化，之后便不再需要，可以交换出去。

#### 0.**物理内存和虚拟内存的关系**

- 物理内存是由于安装内存条而获得的临时存储空间，主要作用是在计算机运行时为操作系统和各种程序提供临时储存。
- **虚拟内存是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存(一个连续完整的地址空间)，它通常是被分割成多个物理内存碎片**，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。

> 通过虚拟地址访问内存有以下优势：
>
> - 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。
> - 程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（通常大小为 4 KB）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。
> - 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存



- 
- 

#### 0.现在有逻辑地址，但是物理地址没有加载到内存中怎么办

![b92a90922053f40978a65b1d60b2dccd.webp](imgs/138.png)

![b8a4acc7653a759efd7ea147c6270e60.webp](imgs/146.png)

![046bec50df60fcdfaaa1a1c548e62e91.webp](imgs/147.png)

#### 0.内存的缺页异常

##### (1)术语规定

- **VA**：Virtual Address 虚拟地址
- **PA**：Physical Address 物理地址
- **MMU**：Memory Manage Unit 内存管理单元
- **TLB**：Translation Lookaside Buffer 旁路快表缓存/地址变换高速缓存
- **PTE**：Page Table Entry 分页表项

##### (2)内存的惰性分配

以32位的Linux系统为例，**每个进程独立拥有4GB的虚拟地址空间，根据局部性原理没有必要也不可能为每个进程分配4GB的物理地址空间。**

64位系统也是一样的道理，只不过空间寻址范围大了很多很多倍，进程的虚拟地址空间会分为几个部分：

![b92a90922053f40978a65b1d60b2dccd.webp](imgs\138.png)

**实际上只有程序运行时用到了才去内存中寻找虚拟地址对应的页帧，找不到才可能进行分配，这就是内存的惰性(延时)分配机制。**

![image-20210428103006849](imgs\139.png)

对于一个运行中的进程来说，不是所有的虚拟地址在物理内存中都有对应的页，如图展示了部分虚拟地址存在对应物理页的情况：

![eb47bd82721b2531b32d368f3f2203b1.webp](imgs\140.png)

虚拟地址空间根据固定大小一般是4KB进行划分，物理内存可以设置不同的页面大小，通常物理页大小和虚拟页大小是一样的，本文按照物理页4KB大小展开。

经过前面的分析，我们将面临一个问题<u>：**如何将虚拟地址准确快速地映射到物理页呢**？</u>

> **>>>高能预警 敲黑板 本段小结<<<**
> **1.** Linux的虚拟地址空间就是**空头支票**，看着很大但是实际对应的物理空间只有很少的一部分。
>
> 2.内存的**惰性分配**是个有效的机制，可以保证内存利用率和服务器利用率，是资源合理配置的方法。
>
> 3.大量的虚拟地址到物理地址的**快速准确地查询转换**是一个难题。

##### (3)CPU如果获取内存中的数据

CPU并不直接和物理内存打交道，而是把地址转换的活外包给了MMU，MMU是一种硬件电路，其速度很快，主要工作是进行内存管理，地址转换只是它承接的业务之一。

![845b27866b6e726f21f63405374e0b73.webp](imgs\141.png)

一起看看MMU是如何搞定地址转换的。576

###### a.MMU和Page Table

**每个进程都会有自己的页表Page Table**，页表存储了**进程中虚拟地址到物理地址的映射关系**，所以就相当于一张地图，MMU收到CPU的虚拟地址之后开始查询页表，确定是否存在映射以及读写权限是否正常，如图：

![3c5bdaeb03b40fad9fec59172c3c768d.webp](imgs\142.png)

对于4GB的虚拟地址且大小为4KB页，一级页表将有2^20个表项，页表占有连续内存并且存储空间大，<u>多级页表可以有效降低页表的存储空间以及内存连续性要求</u>，但是多级页表同时也带来了查询效率问题。

![06c9894abe7eed58c0a24470428ff6a9.webp](imgs\143.png)

我们以2级页表为例，**MMU要先进行两次页表查询确定物理地址**，在确认了权限等问题后，**MMU再将这个物理地址发送到总线，内存收到之后开始读取对应地址的数据并返回。**

![fb14003f0cb593e2ce81418fe99f9d71.webp](imgs\144.png)

MMU在2级页表的情况下进行了2次检索和1次读写，那么当页表变为N级时，就变成了N次检索+1次读写。

可见，页表级数越多查询的步骤越多，对于CPU来说等待时间越长，效率越低，这个问题还需要优化才行。

> **>> 本段小结 敲黑板 划重点 <<**
> 1.页表存在于进程的内存之中，MMU收到虚拟地址之后查询Page Table来获取物理地址。
>
> 2.单级页表对连续内存要求高，于是引入了多级页表，但是**多级页表也是一把双刃剑**，在减少连续存储要求且减少存储空间的同时降低了查询效率。

###### b.MMU和TLB的故事

MMU和TLB的故事就这样开始了...

CPU觉得MMU干活虽然卖力气，但是效率有点低，不太想继续外包给它了，这一下子把MMU急坏了。

MMU于是找来了一些精通统计的朋友，经过一番研究之后发现CPU用的数据经常是一小搓，但是每次MMU都还要重复之前的步骤来检索，害，就知道埋头干活了，也得讲究方式方法呀！

找到瓶颈之后，MMU引入了新武器，江湖人称快表的TLB，别看TLB容量小，但是正式上岗之后干活还真是不含糊。

![16b25e59d94a39a0d6959d059f9c7b6f.webp](imgs\145.png)

当CPU给MMU传新虚拟地址之后，**MMU先去问TLB那边有没有，如果有就直接拿到物理地址发到总线给内存**，齐活。

TLB容量比较小，难免发生Cache Miss，这时候MMU还有保底的老武器页表 Page Table，在页表中找到之后MMU除了把地址发到总线传给内存，还把这条映射关系给到TLB，让它记录一下刷新缓存。

![b8a4acc7653a759efd7ea147c6270e60.webp](imgs\146.png)

TLB容量不满的时候就直接把新记录存储了，当满了的时候就开启了淘汰大法把旧记录清除掉，来保存新记录，彷佛完美解决了问题。

在TLB和Page Table加持之下，CPU感觉最近MMU比较给力了，就问MMU怎么做到的？MMU就一五一十告诉了CPU。

CPU说是个不错的路子，随后说出了自己的建议：TLB还是有点小，缓存不命中也是经常发生的，要不要搞个大的，这样存储更多访问更快？

MMU一脸苦笑说道大哥TLB很贵的，要不你给涨点外包费？话音未落，CPU就说涨工资是不可能了，这辈子都不可能了。

> **>>>高能预警 敲黑板 本段小结<<<**
> \1. CPU要根据用户进程提供的虚拟地址来获取真实数据，但是它并不自己做而是交给了MMU**。**
>
> \2. MMU也是个聪明的家伙，集成了TLB来存储CPU最近常用的页表项来加速寻址，TLB找不到再去全量页表寻址，可以认为TLB是MMU的缓存。
>
> \3. TLB的容量毕竟有限，为此必须依靠Page Table一起完成TLB Miss情况的查询，并且更新到TLB建立新映射关系。



##### (4)缺页异常大揭秘

设想**CPU给MMU的虚拟地址在TLB和Page Table都没有找到对应的物理页帧或者权限不对，该怎么办呢？**

没错，这就是缺页异常Page Fault，**它是一个由硬件中断触发的可以由软件逻辑纠正的错误**。

###### a.PageFault

假如目标内存页在物理内存中**没有对应的页帧或者存在但无对应权限**，**CPU 就无法获取数据**，这种情况下**CPU就会报告一个缺页错误**。

由于CPU没有数据就无法进行计算，CPU罢工了**用户进程也就出现了缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 **Page Fault Handler** 处理。

缺页异常并不可怕，只要CPU要的虚拟地址经过MMU的一番寻址之后没有找到或者找到后无权限，就会出现缺页异常，因此触发异常后的处理流程将是重点内容。

###### b.缺页错误的分类处理

缺页中断会交给PageFaultHandler处理，其根据缺页中断的不同类型会进行不同的处理：

 **Hard Page Fault**

也被称为Major Page Fault，翻译为**硬缺页错误/主要缺页错误**，**这时物理内存中没有对应的页帧**，需要CPU**打开磁盘设备读取到物理内存中，再让MMU建立VA和PA的映射。**

**Soft Page Fault**

也被称为Minor Page Fault，翻译为**软缺页错误/次要缺页错误**，这时**物理内存中是存在对应页帧的**，只不过可能是其他进程调入的，发出缺页异常的进程不知道而已，此时MMU只需要建立映射即可，无需从磁盘读取写入内存，<u>一般出现在多进程共享内存区域。</u>

**Invalid Page Fault**

翻译为**无效缺页错误**，比如进程访问的内存地址越界访问，又比如对空指针解引用内核就会报segment fault错误中断进程直接挂掉。

![046bec50df60fcdfaaa1a1c548e62e91.webp](imgs\147.png)

###### c.缺页错误出现的原因

不同类型的Page Fault出现的原因也不一样，常见的几种原因包括：

 **非法操作访问越界**

这种情况产生的影响也是最大的，也是Coredump的重要来源，比如空指针解引用或者权限问题等都会出现缺页错误。

**使用malloc新申请内存**

malloc机制是延时分配内存，当使用malloc申请内存时并未真实分配物理内存，等到真正开始使用malloc申请的物理内存时发现没有才会启动申请，期间就会出现Page Fault。

**访问数据被swap换出**

物理内存是有限资源，当运行很多进程时并不是每个进程都活跃，对此OS会启动内存页面置换将长时间未使用的物理内存页帧放到swap分区来腾空资源给其他进程，当存在于swap分区的页面被访问时就会触发Page Fault从而再置换回物理内存。

> **>>> 敲黑板 划重点 本段小结：<<<**
> 触发Page Fault的原因可能有很多，归根到底也只有几种大类：
>
> \1. 如使用共享内存区域，没有存储VA->PA的映射但是存在物理页帧的软缺页错误，在Page Table/TLB中建立映射关系即可。
>
> \2. 访问的地址在物理内存中确实不存在，需要从磁盘/swap分区读入才能使用，这种性能影响会比较大，因为磁盘太慢了，尽量使用高性能的SSD来降低延时。
>
> \3. 访问的地址内存非法，缺页错误会升级触发SIGSEGV信号结束进程，这种属于可以导致进程挂掉的一种缺页错误

#### 0.进程切换和线程切换

**首先先来理解一下什么是进程切换？线程切换？**

无论是在多核还是单核系统中，一个CPU看上去都像是在并发的执行多个进程，这是通过处理器在进程间切换来实现的。

- 操作系统实现这种交错执行的机制称为上下文切换；
- 操作系统保持跟踪 进程运行所需要的状态信息，这种状态，就是上下文。

在任何一个时刻，单处理器系统都只能执行一个进程的代码。

> 当操作系统决定要把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文，恢复新进程的上下文，然后将控制权传递到新进程，新进程就会从上次停止的地方开始.

**再来理解一下什么是上下文切换？**

内核为每一个进程维持一个上下文。**上下文就是内核重新启动一个被抢占的线程所需的状态。**包括以下内容：

- 通用目的寄存器
- 浮点寄存器
- 状态寄存器
- 程序计数器：存储下一条将要执行的指令
- 用户栈
- 内核栈
- 各种内核数据结构：比如描绘地址空间的页表，包含有关当前进程信息的进程表，以及包含进程已打开文件的信息的文件表。

在进行进程切换时，涉及**当前执行进程 CPU 环境的保存**及**新调度进程 CPU 环境的设置**，而线程切换时只需保存和**设置少量寄存器内容**，开销很小。

**那么线程切换和进程切换有什么区别呢？**

每个进程都有自己的虚拟地址空间，进程内的所有线程共享进程的虚拟地址空间。那么其中最主要的一个区别在于**进程切换涉及虚拟地址空间的切换而线程不会。**

**那么为什么虚拟地址空间切换会比较耗时呢？**

因为进程有自己的虚拟地址空间，把虚拟地址转换为物理地址需要查找表，页表查找很慢，所以通过有个cache就是TLB。

那么显然每个进程都有自己的页表，那么当进程切换后页表也要进行切换，页表切换后TLB就失效了。所以线程切换要比进程切换要大。





#### 0.逻辑(虚拟)地址和物理地址

正常编程来说我们一般只可能和逻辑地址打交道，比如在C语言中，指针里面存储的数值就可以理解成为内存里面的一个地址，这个地址就是我们说的逻辑地址，**逻辑地址由操作系统决定。**

**物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址**。物理地址就是内存单元真正的地址。

> 比如mov 0x4h8这个是虚拟地址，当我们要对这个虚拟地址里写数据时那么MMU会先判断CPU的**分页状态寄存器里的标志状态**是否被设定，如果被设定那么MMU就会捕获这个**虚拟地址**并在操作系统内核初始化好的内存映射表里查询与之对应的物理地址，并将其转换成真正的**实际物理地址**，然后在对这个实际的物理地址给CPU，在由CPU去执行对应的命令，相反CPU往内存里读数据时比如A进程要读取内存中某个虚拟地址的数据，A进程里的指令给的是**虚拟地**址，MMU首先会检查CPU的分页状态寄存器标志位是否被设置，如果**被设置MMU会捕获这个虚拟地址**并将其转换成相应的物理地址然后提交给**CPU**，在由CPU到内存中去取数据！

#### 1.局部性原理

要想更好地理解虚拟内存技术，必须要了解计算机中著名的**局部性原理**。

早在1968年的时候，就有人指出我们的**程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。**

局部性原理表现在以下两个方面：

- **时间局部性：**如果程序中的某条指令一旦执行，不久以后该指令可能再次执行**；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原子，就是由于在程序中存在着大量的循环操作；**
- **空间局部性：**一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定范围之内，这是因为指令通常是顺序存放、顺序执行的，数据一般是以向量、数组、表等形式簇聚存储的。

> 时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制继承到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了“内存-外存”的两级存储器的结构，利用局部性原理实现高速缓存。

#### 1.逻辑地址 虚拟地址 和物理地址

**（1）逻辑地址转虚拟地址**

**机器语言指令中出现的内存地址，都是逻辑地址**，需要转换成**线性地址**，再经过**MMU(CPU中的内存管理单元)**转换成**物理地址**才能够被访问到。



Linux中**逻辑地址等于线性地址**。为什么这么说呢？因为Linux所有的段（用户代码段、用户数据段、内核代码段、内核数据段）的线性地址都是从 0x00000000 开始，长度4G，这样 **线性地址=逻辑地址+ 0x00000000**，也就是说**逻辑地址等于线性地址**了。

**（2）线性地址转物理地址**

这个大家都知道，那就是通过分页机制，具体的说，就是通过页表查找来对应物理地址。但是页太多了，用线性空间管理不太现实，所以又采用了分段或者说分级管理。





#### 1.虚拟内存是什么？(高频)

> 现代处理器使用的是一种称为虚拟寻址(Virtual Addressing)的寻址方式。**使用虚拟寻址，CPU需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。**实际上完成虚拟地址转换为物理地址转换的硬件是CPU中含有一个被称为内存管理单元(Memory Management Unit，MMU)的硬件。
>
> **如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及同时运行多个程序造成困难。**

虚拟内存的目的是为了让**物理内存**扩充成更大的**逻辑内存**，从而让程序获得更多的可用内存，**虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉(每个进程拥有一片连续完整的内存空间)**；

为了更好的管理内存，操作系统将**内存抽象成地址空间**。**每个程序拥有自己的地址空间**，这**个地址空间被分割成多个块，每一块称之为一页。**

虚拟内存允许程序不用将地址空间中的每一页都映射到内存空间**，当程序引用不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。**



**![image-20210405132933644](imgs/15.png)**



#### 2.分页系统地址映射？

>总结：首先根据虚拟地址=前4位表示页面号 + 后12位表示存储偏移量；这里的4是举例的。16个页需要2^4来表示。
>
>之后根据页面号定位到 页表中，页表前四位中的最后一位表示是否存在内存中，之后用后面的12位来找寻偏移量。

内存管理单元MMU管理着地址空间和物理内存的转换。其中的**页表**(Page table)存储着**页**(程序地址空间)和**页框**(物理内存空间的映射)表。

一个虚拟地址分为两个部分，一部分存储**页面号**，一部分**存储偏移量**。

例子：

下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。

页表项中有一位**标志位**，用来标识包含此数据的页是否在**物理内存中**，如果在的话，就直接做**地址映射**，否则，**抛出缺页中断**，此时页表项也可标识包含**此数据的页是否在调页文件中**(外存)，如果不在则访问违例，程序将会退出，如果在，**页表项会查出此数据页在哪个调页文件中，然后将此数据页调入物理内存，再继续进行地址映射**。

![image-20210405133636921](imgs/16.png)

#### 3.分页内存访问的操作系统如何做到进程的线性地址空间隔离 

操作系统内核上电之后会初始化MMU并将自己映射到 3-4GB的线性地址空间当中；

而当我们通过系统调用比如fork创建进程时，其会在进程描述结构体内集成内存虚拟地址空间的结构体，其内容包含的是当前进程的地址空间页表；

当操作系统进行任务切换时会改写CPU的页表基地址寄存器为当前被运行进程的页表基地址，从而达成切换地址空间范围到响应的进程内存范围的目的。





#### 3.什么是快表和多级页表

快表和多级页表这两个东西分别解决了页表管理中很重要的两个东西。

在分页内存管理中，很重要的两点是：

- **虚拟地址到物理地址的转换要快；**
- **解决虚拟地址空间大，页表也会很大的问题；**

**(1) 快表**

**为了解决虚拟地址到物理地址的转换速度，**操作系统在页表方案的基础上引入了快表来加速虚拟地址到物理地址的转换。可以把快表理解为一种特殊的高速缓冲存储器(Cache), 其中的内容是页表的一部分或全部内容。 但是使用了快表之后可以加速查找并提高指令执行速度。

使用快表之后的地址转换流程是这样的：

- **根据虚拟地址中的页号查找表；**
- 如果该页在快表中，直接从快表中读取相应的物理地址；
- 如果该页不在快表中，就访问**内存中的页表**，再从**页表中得到物理地址**，同时将**页表中的映射表项添加到快表中**；
- 当快表快要填满后，又要登记新表，就按照一定的淘汰策略淘汰掉快表中的一个页。

**(2)多级页表**

**引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景。**

主要是局部性原理，从两方面来节约内存，首先是第一、二级页表可以不存在；其次是二级页表可以不在主存。

主要是因为对于大多数程序来说，其使用到的空间远远未到达4GB，何必去映射不可能用到的空间呢？





#### 4.页面置换算法是什么（高频）

**在程序运行过程中，如果要访问的页面地址不在内存中，就发生缺页中断从而将该页调入内存中。**

但是如果**内存已无空闲空间了，系统就必须从内存中调出一个页面到磁盘对换区中来腾出空间。**

类似于缓存淘汰策略，**在缓存系统中，缓存的大小有限，当有新的缓存到达时，需要淘汰一部分已存在的缓存，这样才有空间存放新的缓存数据。**

页面置换算法的主要目的是使页面置换频率最低(即缺页率最低)。

**(1)最优置换算法：OPT**

是一种理论上的算法，因为我们无法知道后面一个页面多长时间不再被访问。

举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列：

```
7，0，1，2，0，3，0，4，2，3，0，3，2，1，2，0，1，7，0，1
```

开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。

**(2)最近最久未使用（LRU）**

虽然无法知道将来要使用的页面情况，但是可以知道过去使用的页面的情况。

为了实现**LRU**，需要在内存中维护一个所有页面的链表。当一个页面被访问，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未使用的。

**(3)最近未使用（RU）**

每个页面都有两个状态位：R与M，当页面被访问时设置页面的 R=1，当页面被修改时设置M=1.其中R位会被定时清零。

- R=0, M=0;
- R=0, M=1;
- R=1, M=0;
- R=1, M=1

当发生缺页中断时，NRU算法随机从类编号最小的非空类中挑选一个页面将它换出。

NRU优先换出已经被修改的页面(R=1,M=1),而不是被频繁使用的干净页面(R=1,M=0)

**(4)先进先出（FIFO）**

选择换出的页面是最先进入的页面，该算法会将那些被访问的页面换出，导致缺页率升高。

**(5)第二次机会算法**

FIFO算法可能把经常使用的页面置换出去，为了避免这以问题，对该算法做一个简单的修改：

当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。

![image-20210405140557923](./imgs/17.png)

**(6) 时钟**

第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面链接起来，在使用一个指针指向最老的页面。

![image-20210405140648341](imgs\18.png)

>  OPT理论算法、最近最长未使用；最近未使用；FIFO、第二次机会算法、时钟算法。

> 笔试题

一进程刚获得三个主存块的使用权，若该进程访问页面的次序是｛1321215123｝，采用LRU算法时，缺页数是______次。

- ```
  3
  ```

- ```
  4
  ```

- ```
  5
  ```

- ```
  6
  ```

> 5次。

LRU算法：
 1进入—>1
 3进入—>3 1(刚使用的放在前面，未使用的依次后移)
 2进入—>2 3 1
 1进入—>1 2 3
 2进入—>2 1 3
 1进入—>1 2 3
 5进入—>5 1 2 （5不存在，进行页面置换，淘汰旧页2）
 1进入—>1 5 2
 2进入—>2 1 5
 3进入—>3 2 1（3不存在，进行页面置换，淘汰旧页5）
 总共进行了两次页面置换，所以缺页数=3+2=5，缺页率为5/10=0.5；

 FIFO算法：
 1进入—>1
 3进入—>1 3(最先进入内存的放在后面)
 2进入—>1 2 3
 1进入—>2 1 3
 2进入—>1 2 3
 1进入